<h1 align="center">
Pytorch and Transformers Pretrained Models Showcase (Work in Progress)
</h1>

In this repository, I explore various pretrained models using PyTorch and Transformers.

List of models:
- <b>bert_base_uncased</b>: This model is pretrained on the English language (uncased, meaning it does not differentiate between uppercase and lowercase letters). It is accessible on  <a href="https://huggingface.co/bert-base-uncased">Hugging Face</a> and was introduced in this <a href="https://arxiv.org/abs/1810.04805">paper</a> (<a href="https://github.com/google-research/bert">repository</a>). BERT is a transformers model trained on a vast corpus of English data in a self-supervised fashion (i.e., pretrained solely on raw texts, without human labeling).
